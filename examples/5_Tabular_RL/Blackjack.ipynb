{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n",
    "\n",
    "# Blackjack\n",
    "\n",
    "We apply Q-learning to game of Blackjack from the OpenAI gym package.  \n",
    "\n",
    "**Blackjack Rules:**  \n",
    "the rules of the card game Blackjack:\n",
    "- You are dealt two cards (face up). The dealer is dealt two cards (one face up, one face down)\n",
    "- You can ask to get one more card (a hit) which gives you one more card face up or you can stop (stick).\n",
    "- Number cards are worth their face value. King, Queen, Jack are 10. Ace is 1 or 11 (you can choose which).\n",
    "- If your total goes greater then 21 you loose (reward = -1).\n",
    "- If you stick with 21 or below, the dealer then tries to beat or equal your score. \n",
    "- If the dealer gets a score better or equal to you, then you loose (reward = -1).\n",
    "- If the dealer does worst or goes above 21, then you win (reward = +1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning\n",
    "The basic Q-learning update is\n",
    "$$\n",
    "Q(x,a)\n",
    "\\leftarrow\n",
    "Q(x,a)+ \\alpha \\left( r + \\beta \\max_{\\hat a} Q(\\hat x, \\hat a) - Q(x,a)  \\right) \n",
    "$$\n",
    "for (state,action,reward, next state) given by $(x,a,r,\\hat x)$.  \n",
    "(If there is no next state then the maximization above is zero)\n",
    "\n",
    "\n",
    "### Import everything\n",
    "and step up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add location to import stochastic_control\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "# Standard Libraries\n",
    "import numpy as np\n",
    "\n",
    "# Housekeeping modules and settings\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "from IPython.display import clear_output\n",
    "%load_ext autoreload\n",
    "%autoreload 0\n",
    "\n",
    "# The main modules that we need\n",
    "import gym\n",
    "import stochastic_control as sc\n",
    "\n",
    "env = gym.make('Blackjack-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def episode(env,act,train=False):\n",
    "    '''Train for one simulation episode\n",
    "        \n",
    "    #Arguments:\n",
    "        env   -- gym enviroment\n",
    "        train -- function : train(state,action,reward,next_state,done)\n",
    "        act   -- function : act(state)\n",
    "        \n",
    "    #Returns: \n",
    "        Cummulative Episode Reward\n",
    "    '''\n",
    "    state = env.reset()\n",
    "    next_state = None\n",
    "    done = False\n",
    "    Reward = 0.\n",
    "    \n",
    "    while done is False:\n",
    "        action = act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        if train is not False:\n",
    "            train(state,action,reward,next_state,done)\n",
    "        state = next_state\n",
    "        Reward += reward\n",
    "    \n",
    "    return Reward\n",
    "    \n",
    "\n",
    "def test(act,iters,verbose=False):\n",
    "    ''' mean and deviation over a number of iterations \n",
    "    \n",
    "    #Arguments:\n",
    "        act - python function \n",
    "        iters - number of iterations\n",
    "        verbose - use tqdm\n",
    "        \n",
    "    #Returns:\n",
    "        mean_reward, standard_deviation/square_root_number_of_iterations\n",
    "    '''\n",
    "    mean = 0.\n",
    "    mean_sqd = 0.\n",
    "    \n",
    "    if verbose :\n",
    "        for t in tqdm(range(iters)):\n",
    "            reward = episode(env,act)\n",
    "            mean += (reward-mean)/(t+1)\n",
    "            mean_sqd += (reward**2 - mean_sqd)/(t+1)            \n",
    "    else: \n",
    "        for t in (range(iters)):\n",
    "            reward = episode(env,act)\n",
    "            mean += (reward-mean)/(t+1)\n",
    "            mean_sqd += (reward**2 - mean_sqd)/(t+1)    \n",
    "        \n",
    "    return mean, np.sqrt((mean_sqd-mean**2)/iters)\n",
    "\n",
    "def blackjack_print(Q):\n",
    "    '''helper function to print nicely'''\n",
    "    \n",
    "    #print table for no usable ace\n",
    "    dealer = range(1,11)\n",
    "    player_false_ace = range(4,22)\n",
    "    print('WITH NO ACE')\n",
    "    print('player\\dealer', end='\\n\\t')\n",
    "    for dlr in dealer:\n",
    "        print(dlr, end = ' ')\n",
    "    print('\\n')\n",
    "\n",
    "    for plyr in player_false_ace:\n",
    "        print(plyr, end='\\t')\n",
    "        for dlr in dealer:\n",
    "            print(Q.argmax((plyr,dlr,False)), end = ' ')\n",
    "        print('')\n",
    "    print('\\n')\n",
    "    \n",
    "    #print table for no usable ace\n",
    "    player_true_ace = range(12,22)\n",
    "    print('WITH ACE')\n",
    "    print('player\\dealer', end='\\n\\t')\n",
    "    for dlr in dealer:\n",
    "        print(dlr, end = ' ')\n",
    "    print('\\n')\n",
    "\n",
    "    for plyr in player_true_ace:\n",
    "        print(plyr, end='\\t')\n",
    "        for dlr in dealer:\n",
    "            print(Q.argmax((plyr,dlr,True)), end = ' ')\n",
    "        print(\"\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Basic Working Example\n",
    "We set a fixed learning rate (0.0001)  \n",
    "We set chose actions (hit/stick) uniformly at random  \n",
    "We run for 20 million episodes (takes a few minutes -- yawn!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000000/20000000 [17:34<00:00, 18959.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WITH NO ACE\n",
      "player\\dealer\n",
      "\t1 2 3 4 5 6 7 8 9 10 \n",
      "\n",
      "4\t1 1 1 1 1 1 1 1 1 1 \n",
      "5\t1 1 1 1 1 1 1 1 1 1 \n",
      "6\t1 1 1 1 1 1 1 1 1 1 \n",
      "7\t1 1 1 1 1 1 1 1 1 1 \n",
      "8\t1 1 1 1 1 1 1 1 1 1 \n",
      "9\t1 1 1 1 1 1 1 1 1 1 \n",
      "10\t1 1 1 1 1 1 1 1 1 1 \n",
      "11\t1 1 1 1 1 1 1 1 1 1 \n",
      "12\t1 0 0 0 0 0 1 1 1 1 \n",
      "13\t1 0 0 0 0 0 1 1 1 1 \n",
      "14\t1 0 0 0 0 0 1 1 1 1 \n",
      "15\t1 0 0 0 0 0 0 1 1 1 \n",
      "16\t1 0 0 0 0 0 0 0 0 0 \n",
      "17\t0 0 0 0 0 0 0 0 0 0 \n",
      "18\t0 0 0 0 0 0 0 0 0 0 \n",
      "19\t0 0 0 0 0 0 0 0 0 0 \n",
      "20\t0 0 0 0 0 0 0 0 0 0 \n",
      "21\t0 0 0 0 0 0 0 0 0 0 \n",
      "\n",
      "\n",
      "WITH ACE\n",
      "player\\dealer\n",
      "\t1 2 3 4 5 6 7 8 9 10 \n",
      "\n",
      "12\t1 1 1 1 1 1 1 1 1 1 \n",
      "13\t1 1 1 1 1 1 1 1 1 1 \n",
      "14\t1 1 1 1 1 1 1 1 1 1 \n",
      "15\t1 1 1 1 1 1 1 1 1 1 \n",
      "16\t1 1 1 1 1 1 1 1 1 1 \n",
      "17\t1 1 1 1 1 0 1 1 1 1 \n",
      "18\t1 0 0 0 0 0 0 0 1 1 \n",
      "19\t1 0 0 0 0 0 0 0 0 0 \n",
      "20\t0 0 0 0 0 0 0 0 0 0 \n",
      "21\t0 0 0 0 0 0 0 0 0 0 \n"
     ]
    }
   ],
   "source": [
    "# Get Q-function and define action and training functions\n",
    "Q = sc.Q_Learn(lr=0.00001)\n",
    "act = lambda state : env.action_space.sample()\n",
    "train = lambda s,a,r,ns,d : Q.train(s,a,r,ns,d)\n",
    "\n",
    "# main loop\n",
    "iterations = range(20000000)\n",
    "for _ in tqdm(iterations):\n",
    "    episode(env,act,train)\n",
    "blackjack_print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000000/10000000 [08:45<00:00, 19020.20it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'mean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-6b1bbb9166b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10000000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mean score:\\t'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m' +/- '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_sqd\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m#print('std dev score:\\t',np.round(np.sqrt((mean_sqd-mean**2)/t),5))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mean' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test the policy\n",
    "Q_opt = Q\n",
    "iterations = range(10000000)\n",
    "act = lambda s : Q_opt.act(s,actions=[0,1])\n",
    "\n",
    "test(act,10000000,verbose=True)\n",
    "    \n",
    "print('mean score:\\t',mean,' +/- ',np.round(np.sqrt((mean_sqd-mean**2)/t),5))\n",
    "#print('std dev score:\\t',np.round(np.sqrt((mean_sqd-mean**2)/t),5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With $\\frac{1}{t}$ the learning rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Q-function and define action and training functions\n",
    "Q = sc.Q_Learn()\n",
    "act = lambda state : Q.act(state,explore=1.0,actions=[0,1])\n",
    "train = lambda s,a,r,ns,d: Q.train(s,a,r,ns,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time= 2040.3325080871582\n",
      "WITH NO ACE\n",
      "player\\dealer\n",
      "\t1 2 3 4 5 6 7 8 9 10 \n",
      "\n",
      "4\t1 1 1 1 1 1 1 1 1 1 \n",
      "5\t1 1 1 1 1 1 1 1 1 1 \n",
      "6\t1 1 1 1 1 0 1 1 1 1 \n",
      "7\t1 1 1 1 1 1 1 1 1 1 \n",
      "8\t1 1 1 1 1 1 1 1 1 1 \n",
      "9\t1 1 1 1 1 1 1 1 1 1 \n",
      "10\t1 1 1 1 1 1 1 1 1 1 \n",
      "11\t1 1 1 1 1 1 1 1 1 1 \n",
      "12\t1 1 1 1 0 0 1 1 1 1 \n",
      "13\t1 0 0 0 0 0 1 1 1 1 \n",
      "14\t1 0 0 0 0 0 1 1 1 1 \n",
      "15\t1 0 0 0 0 0 1 1 1 1 \n",
      "16\t1 0 0 0 0 0 1 1 1 1 \n",
      "17\t0 0 0 0 0 0 0 0 0 0 \n",
      "18\t0 0 0 0 0 0 0 0 0 0 \n",
      "19\t0 0 0 0 0 0 0 0 0 0 \n",
      "20\t0 0 0 0 0 0 0 0 0 0 \n",
      "21\t0 0 0 0 0 0 0 0 0 0 \n",
      "\n",
      "\n",
      "WITH ACE\n",
      "player\\dealer\n",
      "\t1 2 3 4 5 6 7 8 9 10 \n",
      "\n",
      "12\t1 1 1 1 1 1 1 1 0 1 \n",
      "13\t1 1 1 1 1 1 1 1 1 1 \n",
      "14\t1 1 1 1 1 1 1 1 1 1 \n",
      "15\t1 1 1 1 1 1 1 1 1 1 \n",
      "16\t1 1 1 1 1 1 1 1 1 1 \n",
      "17\t1 1 1 1 1 1 1 1 1 1 \n",
      "18\t1 1 0 1 1 0 0 0 1 1 \n",
      "19\t0 1 0 1 0 0 0 0 0 0 \n",
      "20\t0 0 0 0 0 0 0 0 0 0 \n",
      "21\t0 0 0 0 0 0 0 0 0 0 \n"
     ]
    }
   ],
   "source": [
    "# main loop\n",
    "iterations = range(20000000)\n",
    "for t in tqdm(iterations):\n",
    "    Q.lr = 1000/(t+1) \n",
    "    episode(env,act,train)\n",
    "blackjack_print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test the policy\n",
    "Q_opt = Q\n",
    "iterations = range(10000000)\n",
    "act = lambda s : Q_opt.act(s,actions=[0,1])\n",
    "test(act,10000000,verbose=True)\n",
    "    \n",
    "print('mean score:\\t',mean,' +/- ',np.round(np.sqrt((mean_sqd-mean**2)/t),5))\n",
    "#print('std dev score:\\t',np.round(np.sqrt((mean_sqd-mean**2)/t),5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating Performance\n",
    "We want to investigate the Performance of Q-learning and find good parameters.  \n",
    "\n",
    "Parameters we need to consider are:\n",
    "- How long to train for\n",
    "- Learning rate\n",
    "- exploration probability\n",
    "- reducing the exploration probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching Parameters one-by-one\n",
    "**Training time** Let's fix a good time budget for each training run. Eg. between 1 to 2 seconds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A good number of iterations is:  32768\n"
     ]
    }
   ],
   "source": [
    "# again random actions and the Q-function for learning\n",
    "Q = sc.Q_Learn(lr=0.01)\n",
    "act = lambda state : env.action_space.sample()\n",
    "train = lambda s,a,r,ns,d : Q.train(s,a,r,ns,d)\n",
    "\n",
    "max_run_time = 10. \n",
    "current_run_time = 0.\n",
    "iterations = 1 \n",
    "\n",
    "while current_run_time < 1. :\n",
    "    iterations *= 2 \n",
    "    tic = time()\n",
    "    for _ in tqdm(range(iterations)):\n",
    "        episode(env,act,train)\n",
    "    toc = time()\n",
    "    current_run_time = toc-tic\n",
    "clear_output(wait=True)\n",
    "print('A good number of iterations is: ',iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay so lets round down and take 100,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = 30000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning rate**: Let's find a good learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0 -0.3712 +/- 0.005205\n",
      "1.0 -0.207767 +/- 0.005445\n",
      "0.1 -0.0806 +/- 0.005498\n",
      "0.01 -0.054133 +/- 0.005488\n",
      "best learning rate is : 0.01\n"
     ]
    }
   ],
   "source": [
    "alphas = [1. * 10**(-t) for t in range(-1,3)]\n",
    "Qs ={ alpha : sc.Q_Learn(lr=alpha) for alpha in alphas} \n",
    "\n",
    "def find_alpha(Q_dict,alpha_dict,iters):\n",
    "    ''' Finds the best learning rate\n",
    "    '''\n",
    "    max_mean = -1*np.inf\n",
    "    max_alpha = alphas[0]\n",
    "\n",
    "    for alpha in alpha_dict:\n",
    "        # step up Q-function\n",
    "        act_training = lambda state : env.action_space.sample()\n",
    "        train = lambda s,a,r,ns,d : Q_dict[alpha].train(s,a,r,ns,d,lr=alpha)\n",
    "\n",
    "        for _ in (range(iters)):\n",
    "            episode(env,act_training,train)\n",
    "\n",
    "        act_testing = lambda state : Q_dict[alpha].act(state)\n",
    "        mean, std = test(act_testing,iters)\n",
    "        print(np.round(alpha,6),np.round(mean,6),'+/-',np.round(std,6))\n",
    "\n",
    "        if mean > max_mean :       \n",
    "            max_mean = mean \n",
    "            max_alpha = alpha\n",
    "    \n",
    "    return max_alpha, max_mean\n",
    "\n",
    "alpha_star, _ = find_alpha(Qs,alphas,iters)\n",
    "print('best learning rate is :',alpha_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot their performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAD8CAYAAACGsIhGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkoWIiKSk5KFiIjkpGQhIiI5/T8lhqvEoxKNSQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-43596ff2b408>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mQs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0malp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mmean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0malp\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0malp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0miters\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-0ba3fcc7e197>\u001b[0m in \u001b[0;36mepisode\u001b[0;34m(env, act, train)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/gym/envs/toy_text/blackjack.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# hit: add a card to players hand and return\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdraw_card\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_bust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                 \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/gym/envs/toy_text/blackjack.py\u001b[0m in \u001b[0;36mdraw_card\u001b[0;34m(np_random)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdraw_card\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeck\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rplt= sc.rolling_plot(5)\n",
    "\n",
    "# alphas = [1. * 10**(-2*t) for t in range(-1,3)]\n",
    "alphas = [0.1, 0.05, 0.01, 0.001,0.0000001]\n",
    "Qs ={ alpha : sc.Q_Learn(lr=alpha) for alpha in alphas} \n",
    "\n",
    "mean = { alpha : 0 for alpha in alphas} \n",
    "iters=5000\n",
    "\n",
    "for i in tqdm(range(1000)):\n",
    "    for idx, alp in enumerate(alphas):  \n",
    "        act_training = lambda state : env.action_space.sample()\n",
    "        train = lambda s,a,r,ns,d : Qs[alp].train(s,a,r,ns,d,lr=alp)\n",
    "        for t in (range(iters)):\n",
    "            \n",
    "            reward = episode(env,Qs[alp].policy(actions=[0,1]))\n",
    "            mean[alp] += (reward-mean[alp])/(i*iters+t+1)\n",
    "            \n",
    "        for t in (range(iters)):\n",
    "            episode(env,act_training,train)\n",
    "            \n",
    "\n",
    "\n",
    "        if i > 10 :\n",
    "            rplt.plot(mean[alp],idx)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blackjack_print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now Repeat:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [03:29<00:00, 21.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha star =  1.0000000000000002e-10 , mean = -0.047580000000000296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "repetitions = 10\n",
    "iters = 50000\n",
    "\n",
    "alpha_star = 1.\n",
    "Q_star = sc.Q_Learn()\n",
    "\n",
    "def Q_alph_update(Q_star,alpha_star,iters):\n",
    "    print('---finding next alpha---')\n",
    "    alphas = [alpha_star * 10**(-t) for t in range(-1,3)]\n",
    "    Qs = { alp : Q_star.copy() for alp in alphas }\n",
    "\n",
    "    alpha_star, score = find_alpha(Qs,alphas,iters)\n",
    "    Q_star = Qs[alpha_star]\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    print('alpha star = ',alpha_star, ', mean =', score)\n",
    "    return Q_star, alpha_star, score\n",
    "\n",
    "Best_Qs = []\n",
    "for _ in tqdm(range(repetitions)):\n",
    "    Q_star, alpha_star, score = Q_alph_update(Q_star,alpha_star,iters)\n",
    "    Best_Qs.append(Q_star.copy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WITH NO ACE\n",
      "player\\dealer\n",
      "\t1 2 3 4 5 6 7 8 9 10 \n",
      "\n",
      "4\t1 1 1 0 1 1 1 1 1 1 \n",
      "5\t1 1 0 0 0 1 1 1 1 1 \n",
      "6\t1 1 1 1 1 1 1 1 1 1 \n",
      "7\t1 1 1 1 1 1 1 1 1 1 \n",
      "8\t1 1 1 1 1 1 1 1 1 1 \n",
      "9\t1 1 1 1 1 1 1 1 1 1 \n",
      "10\t1 1 1 1 1 1 1 1 1 1 \n",
      "11\t1 1 1 1 1 1 1 1 1 1 \n",
      "12\t1 1 0 1 0 0 1 1 1 1 \n",
      "13\t1 0 0 0 0 0 1 1 1 1 \n",
      "14\t1 0 0 0 0 0 1 1 0 1 \n",
      "15\t1 0 0 0 0 0 1 1 1 1 \n",
      "16\t1 0 0 0 0 0 1 0 1 0 \n",
      "17\t1 0 0 0 0 0 0 0 0 0 \n",
      "18\t0 0 0 0 0 0 0 0 0 0 \n",
      "19\t0 0 0 0 0 0 0 0 0 0 \n",
      "20\t0 0 0 0 0 0 0 0 0 0 \n",
      "21\t0 0 0 0 0 0 0 0 0 0 \n",
      "\n",
      "\n",
      "WITH ACE\n",
      "player\\dealer\n",
      "\t1 2 3 4 5 6 7 8 9 10 \n",
      "\n",
      "12\t1 1 1 1 1 1 1 1 1 1 \n",
      "13\t1 1 1 1 1 1 1 1 1 1 \n",
      "14\t1 1 1 1 1 1 1 1 1 1 \n",
      "15\t1 1 1 1 1 1 1 1 1 1 \n",
      "16\t1 1 1 1 1 1 1 1 1 1 \n",
      "17\t1 0 0 0 0 1 0 1 1 1 \n",
      "18\t1 0 0 0 0 0 0 1 0 1 \n",
      "19\t0 0 0 0 0 0 0 0 0 0 \n",
      "20\t0 0 0 0 0 0 0 0 0 0 \n",
      "21\t0 0 0 0 0 0 0 0 0 0 \n"
     ]
    }
   ],
   "source": [
    "blackjack_print(Best_Qs[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exploration**: Now we investigate exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 -0.088933 +/- 0.005486\n",
      "0.2 -0.089633 +/- 0.005524\n",
      "0.30000000000000004 -0.097567 +/- 0.005505\n",
      "0.4 -0.0743 +/- 0.005518\n",
      "0.5 -0.082367 +/- 0.005498\n",
      "0.6 -0.061833 +/- 0.005506\n",
      "0.7000000000000001 -0.060467 +/- 0.005509\n",
      "0.8 -0.0596 +/- 0.005525\n",
      "0.9 -0.0628 +/- 0.005498\n",
      "best exploration prob is : 0.8\n"
     ]
    }
   ],
   "source": [
    "iters = 30000\n",
    "epsilons = np.arange(0.1,1,0.1)\n",
    "\n",
    "# set up Q-functions actions for training and testing and training function\n",
    "Qs = dict()\n",
    "for eps in epsilons:\n",
    "    Qs[eps] = sc.Q_Learn(lr=alpha_star)\n",
    "\n",
    "def find_epsilon(Q_dict,eps_dict,iters):\n",
    "    act_train = dict()\n",
    "    act_test = dict()\n",
    "    trains = dict()\n",
    "    for eps in eps_dict:\n",
    "        act_train[eps] = lambda state : Q_dict[eps].act(state,explore=eps,actions=[0,1])\n",
    "        act_test[eps] = lambda state : Q_dict[eps].act(state,actions=[0,1])\n",
    "        trains[eps] = lambda s,a,r,ns,d : Q_dict[eps].train(s,a,r,ns,d)\n",
    "\n",
    "    max_mean = -1*np.inf\n",
    "    max_eps = epsilons[0]\n",
    "\n",
    "    for eps in eps_dict:\n",
    "\n",
    "        for _ in (range(iters)):\n",
    "            episode(env,act_train[eps],trains[eps])\n",
    "\n",
    "        mean, std = test(act_test[eps],iters)\n",
    "        print(eps,np.round(mean,6),'+/-',np.round(std,6)) \n",
    "\n",
    "        if mean > max_mean :       \n",
    "            max_mean = mean \n",
    "            max_eps = eps\n",
    "    \n",
    "    return max_eps, max_mean\n",
    "    \n",
    "eps_star, _ = find_epsilon(Qs,epsilons,iters)\n",
    "print('best exploration prob is :',eps_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now repeat:** Make sure you keep track of the best solutions found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [14:46<00:00, 87.77s/it]\n"
     ]
    }
   ],
   "source": [
    "repetitions = 10\n",
    "iters = 100000\n",
    "\n",
    "alpha_star = 1.\n",
    "epsilon_star = 1.\n",
    "Q_star = sc.Q_Learn()\n",
    "\n",
    "def Q_alph_update(Q_star,alpha_star,iters):\n",
    "    print('---finding alpha---')\n",
    "    alphas = [alpha_star * 10**(-t) for t in range(-1,3)]\n",
    "    Qs = { alp : Q_star.copy() for alp in alphas }\n",
    "\n",
    "    alpha_star, score = find_alpha(Qs,alphas,iters)\n",
    "    Q_star = Qs[alpha_star]\n",
    "    print('alpha star = ',alpha_star)\n",
    "    \n",
    "    return Q_star, alpha_star, score\n",
    "\n",
    "def Q_eps_update(Q_star,epsilon_star,iters):\n",
    "    print('---finding epsilon---')\n",
    "    epsilons = [epsilon_star * m for m in [0.2,0.4,0.6,0.8,1.]]\n",
    "    Qs = { eps : Q_star.copy() for eps in epsilons }\n",
    "    \n",
    "    epsilon_star, score = find_epsilon(Qs,epsilons,iters)\n",
    "    Q_star = Qs[epsilon_star]\n",
    "    print('epsilon star = ',epsilon_star)\n",
    "    \n",
    "    return Q_star, epsilon_star, score\n",
    "\n",
    "Best_Qs = []\n",
    "for _ in tqdm(range(repetitions)):\n",
    "    Q_star, alpha_star, score = Q_alph_update(Q_star,alpha_star,iters)\n",
    "    Best_Qs.append(Q_star.copy())\n",
    "    Q_star, epsilon_star, score = Q_eps_update(Q_star,epsilon_star,iters)\n",
    "    Best_Qs.append(Q_star.copy())\n",
    "    clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now:** let's evaluate of best solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -0.05539000000000009 +/- 0.0030132027468970563\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'max_mean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-8c8aea3f500d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'+/-'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_sqd\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mmean\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmax_mean\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mmax_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mmax_alpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'max_mean' is not defined"
     ]
    }
   ],
   "source": [
    "for idx, Qs in enumerate(Best_Qs):\n",
    "    act = lambda s : Qs.act(s)\n",
    "    mean = 0.\n",
    "    mean_sqd = 0.\n",
    "    for t in (range(100000)):\n",
    "        reward = episode(env,act)\n",
    "        mean += (reward-mean)/(t+1)\n",
    "        mean_sqd += (reward**2 - mean_sqd)/(t+1)\n",
    "\n",
    "    print(idx,mean,'+/-',np.sqrt(mean_sqd/t)) \n",
    "\n",
    "    if mean > max_mean :       \n",
    "        max_mean = mean \n",
    "        max_alpha = alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blackjack_print(Best_Qs[16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [lambda s,a,r,ns,d : Q_dict[new].train(s,a,r,ns,d) for new in [0,1]]\n",
    "acts =[None,None]\n",
    "\n",
    "means = [0., 0.]\n",
    "means_sqd = [0., 0.]\n",
    "sigma = [0.,0.]\n",
    "\n",
    "tic = time()\n",
    "toc = 0 \n",
    "while toc < 600:\n",
    "    \n",
    "    print('training')\n",
    "    for new in [0,1]:\n",
    "        for _ in (range(iters)):\n",
    "            episode(env,rand_act,train[new])\n",
    "\n",
    "    print('testing')   \n",
    "    for new in [0,1]:\n",
    "        acts[new] = lambda state : Qs[new].act(state,actions=[0,1])\n",
    "        for t in (range(iters)):\n",
    "            reward = episode(env,acts[new],train[new])\n",
    "            means[new] += (reward-means[new])/(t+1)\n",
    "            means_sqd[new] += (reward**2 - means_sqd[new])/(t+1)\n",
    "            \n",
    "        sigma[new] = np.sqrt(means_sqd[new]/(t+1))\n",
    "\n",
    "        print(Qs[new].lr,means[new],'+/-',np.sqrt(means_sqd[new]/(t+1)))     \n",
    "\n",
    "    if means[1]-sigma[1] > means[0]+sigma[0]:\n",
    "        print('update learning rate to', Qs[1].lr)\n",
    "        Qs[0] = Qs[1]\n",
    "        Qs[1] = Qs[1].copy()\n",
    "        Qs[1].lr /= 2\n",
    "        \n",
    "        means = [0., 0.]\n",
    "        means_sqd = [0., 0.]\n",
    "    elif means[0]-sigma[0] > means[1]+sigma[1] :\n",
    "        print('keep current rate')\n",
    "        Qs[1] = Qs[0].copy()\n",
    "        \n",
    "    else:\n",
    "        iters = int(1.1*iters)\n",
    "        print('i training to ',iters)\n",
    "    \n",
    "    toc = time()-tic\n",
    "    print('time= ',toc)\n",
    "        \n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blackjack_print(Qs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = 1000\n",
    "alphas = [1., .5]\n",
    "lr_factor = 4\n",
    "Qs = [sc.Q_Learn(1.), sc.Q_Learn(.5)]\n",
    "rand_act = lambda state : env.action_space.sample()\n",
    "train = [lambda s,a,r,ns,d : Qs[new].train(s,a,r,ns,d) for new in [0,1]]\n",
    "acts =[None,None]\n",
    "\n",
    "means = [0., 0.]\n",
    "means_sqd = [0., 0.]\n",
    "sigma = [0.,0.]\n",
    "N = [0, 0]\n",
    "\n",
    "tic = time()\n",
    "toc = 0 \n",
    "\n",
    "while toc < 60*10:\n",
    "    \n",
    "\n",
    "    for new in [0,1]:\n",
    "        acts[new] = lambda state : Qs[new].act(state,actions=[0,1])\n",
    "        for _ in (range(iters)):\n",
    "            episode(env,rand_act,train[new])   \n",
    "            reward = episode(env,acts[new],train[new])\n",
    "            means[new] += (reward-means[new])/(N[new]+1)\n",
    "            means_sqd[new] += (reward**2 - means_sqd[new])/(N[new]+1)\n",
    "            sigma[new] = np.sqrt(means_sqd[new]/(N[new]+1))\n",
    "            N[new]+= 1\n",
    "\n",
    "        print(Qs[new].lr,means[new],'+/-',np.sqrt(means_sqd[new]/(N[new]+1)))     \n",
    "\n",
    "    if means[1]-sigma[1] > means[0]+sigma[0]:\n",
    "        print('update learning rate to', Qs[1].lr)\n",
    "        \n",
    "        Qs[0] = Qs[1]\n",
    "        Qs[1] = Qs[1].copy()\n",
    "        \n",
    "        Qs[1].lr /= lr_factor\n",
    "        \n",
    "        N[0] = N[1]\n",
    "        N[1] = 0\n",
    "        \n",
    "        means[0] = means[1]\n",
    "        means_sqd[0] = means_sqd[1]\n",
    "\n",
    "        means[1] = 0.\n",
    "        means_sqd[1] = 0.\n",
    "\n",
    "    elif means[0]-sigma[0] > means[1]+sigma[1] :\n",
    "        print('keep current rate')\n",
    "        \n",
    "        Qs[1] = Qs[0].copy()\n",
    "        Qs[1].lr = Qs[0].lr/ lr_factor\n",
    "        \n",
    "        N[1] = 0\n",
    "\n",
    "    else:\n",
    "        iters = int(2*iters)\n",
    "        print('update iterations training to ',iters)\n",
    "\n",
    "    toc = time()-tic\n",
    "    #clear_output(wait=True)\n",
    "    print('-----------------')\n",
    "    print('time = ',toc)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Countinous update\n",
    "'''\n",
    "\n",
    "iters = 100\n",
    "alphas = [1., .25]\n",
    "lr_factor = 4.\n",
    "Qs = [sc.Q_Learn(alphas[0]), sc.Q_Learn(alphas[1])]\n",
    "rand_act = lambda state : env.action_space.sample()\n",
    "train = [lambda s,a,r,ns,d : Qs[new].train(s,a,r,ns,d) for new in [0,1]]\n",
    "acts =[None,None]\n",
    "\n",
    "means = [0., 0.]\n",
    "means_sqd = [0., 0.]\n",
    "sigma = [0.,0.]\n",
    "N = [0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time()\n",
    "toc = 0 \n",
    "it = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.805 1200.0\n",
      "1.5e-05 -0.045545 +/- 0.000996\n",
      "4e-06 -0.046406 +/- 0.000996\n"
     ]
    }
   ],
   "source": [
    "while toc < 20*60:    \n",
    "    it += 1\n",
    "    \n",
    "    if it % 100 == 0 :\n",
    "        print(it/100000, np.round(time()-tic,0))\n",
    "        for new in [0,1]:\n",
    "            print(np.round(Qs[new].lr,6),\n",
    "                  np.round(means[new],6),\n",
    "                  '+/-',np.round(np.sqrt(means_sqd[new]/(N[new]+1)),6))  \n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    for new in [0,1]:\n",
    "        acts[new] = lambda state : Qs[new].act(state,actions=[0,1])\n",
    "\n",
    "        episode(env,rand_act,train[new])   \n",
    "        reward = episode(env,acts[new],train[new])\n",
    "        means[new] += (reward-means[new])/(N[new]+1)\n",
    "        means_sqd[new] += (reward**2 - means_sqd[new])/(N[new]+1)\n",
    "        sigma[new] = np.sqrt(means_sqd[new]/(N[new]+1))\n",
    "        N[new]+= 1\n",
    "\n",
    "        if means[1]-1*sigma[1] > means[0]+ 1*sigma[0]:\n",
    "\n",
    "            Qs[0] = Qs[1]\n",
    "            Qs[1] = Qs[1].copy()\n",
    "\n",
    "            Qs[1].lr /= lr_factor\n",
    "\n",
    "            N[0] = N[1]\n",
    "            N[1] = 0\n",
    "\n",
    "            means[0] = means[1]\n",
    "            means_sqd[0] = means_sqd[1]\n",
    "\n",
    "            means[1] = means[1]\n",
    "            means_sqd[1] = 100.\n",
    "\n",
    "    toc = time()-tic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WITH NO ACE\n",
      "player\\dealer\n",
      "\t1 2 3 4 5 6 7 8 9 10 \n",
      "\n",
      "4\t1 1 1 1 1 1 1 1 1 1 \n",
      "5\t1 1 1 1 1 1 1 1 1 1 \n",
      "6\t1 1 1 1 1 1 1 1 1 1 \n",
      "7\t1 1 1 1 1 1 1 1 1 1 \n",
      "8\t1 1 1 1 1 1 1 1 1 1 \n",
      "9\t1 1 1 1 1 1 1 1 1 1 \n",
      "10\t1 1 1 1 1 1 1 1 1 1 \n",
      "11\t1 1 1 1 1 1 1 1 1 1 \n",
      "12\t1 1 1 0 1 0 1 1 1 1 \n",
      "13\t1 0 0 0 0 0 1 1 1 1 \n",
      "14\t1 0 0 0 0 0 1 1 1 1 \n",
      "15\t1 0 0 0 0 0 1 1 1 1 \n",
      "16\t1 0 0 0 0 0 1 1 1 1 \n",
      "17\t0 0 0 0 0 0 0 0 0 0 \n",
      "18\t0 0 0 0 0 0 0 0 0 0 \n",
      "19\t0 0 0 0 0 0 0 0 0 0 \n",
      "20\t0 0 0 0 0 0 0 0 0 0 \n",
      "21\t0 0 0 0 0 0 0 0 0 0 \n",
      "\n",
      "\n",
      "WITH ACE\n",
      "player\\dealer\n",
      "\t1 2 3 4 5 6 7 8 9 10 \n",
      "\n",
      "12\t1 1 1 1 1 1 1 1 1 1 \n",
      "13\t1 1 1 1 1 1 1 1 1 1 \n",
      "14\t1 1 1 1 1 1 1 1 1 1 \n",
      "15\t1 1 1 1 1 1 1 1 1 1 \n",
      "16\t1 1 1 1 1 1 1 1 1 1 \n",
      "17\t1 1 1 1 1 1 1 1 1 1 \n",
      "18\t0 0 0 0 0 0 0 0 1 1 \n",
      "19\t0 0 0 0 0 0 0 0 0 0 \n",
      "20\t0 0 0 0 0 0 0 0 0 0 \n",
      "21\t0 0 0 0 0 0 0 0 0 0 \n"
     ]
    }
   ],
   "source": [
    "blackjack_print(Qs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "alphas = [1. * 10**(-t) for t in range(-1,9)]\n",
    "Q_dict = dict()\n",
    "for alpha in alphas:\n",
    "    Q_dict[alpha] = sc.Q_Learn(lr=alpha)\n",
    "\n",
    "for alpha in alphas:\n",
    "    # step up Q-function\n",
    "    act_training = lambda state : env.action_space.sample()\n",
    "    train = lambda s,a,r,ns,d : Q_dict[alpha].train(s,a,r,ns,d)\n",
    "\n",
    "    for _ in tqdm(iters):\n",
    "        episode(env,train,act_training)\n",
    "    \n",
    "    act_testing = lambda state : Q_dict[alpha].act(state)\n",
    "    \n",
    "    mean = 0.\n",
    "    mean_sqd = 0.\n",
    "    for t in tqdm(iters):\n",
    "        reward = episode(env,train,act_testing)\n",
    "        mean += (reward-mean)/(t+1)\n",
    "        mean_sqd += (reward**2 - mean_sqd)/(t+1)\n",
    "        \n",
    "    print(alpha,mean,'+/-',np.sqrt(mean_sqd/t))   \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "epss = [1., .5 , 0.1, 0.05, 0.01]\n",
    "\n",
    "Q_dict = dict()\n",
    "for eps in epss:\n",
    "    Q_dict[eps] = sc.Q_Learn(lr=alpha)\n",
    "\n",
    "for eps in epss:\n",
    "    act_training = lambda state :  Q_dict[eps].act(state,eps,actions=[0,1])\n",
    "    train = lambda s,a,r,ns,d : Q_dict[eps].train(s,a,r,ns,d)\n",
    "\n",
    "    for _ in tqdm(iters):\n",
    "        episode(env,act_training,train)\n",
    "\n",
    "    act_testing = lambda state : Q_dict[eps].act(state)\n",
    "\n",
    "    mean = 0.\n",
    "    mean_sqd = 0.\n",
    "    for t in tqdm(iters):\n",
    "        reward = episode(env,act_testing,train)\n",
    "        mean += (reward-mean)/(t+1)\n",
    "        mean_sqd += (reward**2 - mean_sqd)/(t+1)\n",
    "\n",
    "    print(eps,mean,'+/-',np.sqrt(mean_sqd/t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = range(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [1. * 10**(-t) for t in range(-1,9)]\n",
    "Q_dict_old = Q_dict\n",
    "Q_dict = dict()\n",
    "for alpha in alphas:\n",
    "    Q_dict[alpha] = Q_dict_old[1.].copy()\n",
    "\n",
    "for alpha in alphas:\n",
    "    # step up Q-function\n",
    "    act_training = lambda state : env.action_space.sample()\n",
    "    train = lambda s,a,r,ns,d : Q_dict[alpha].train(s,a,r,ns,d)\n",
    "\n",
    "    for _ in tqdm(iters):\n",
    "        episode(env,act_training,train)\n",
    "    \n",
    "    act_testing = lambda state : Q_dict[alpha].act(state)\n",
    "    \n",
    "    mean = 0.\n",
    "    mean_sqd = 0.\n",
    "    for t in tqdm(iters):\n",
    "        reward = episode(env,act_testing,train)\n",
    "        mean += (reward-mean)/(t+1)\n",
    "        mean_sqd += (reward**2 - mean_sqd)/(t+1)\n",
    "        \n",
    "    print(alpha,mean,'+/-',np.sqrt(mean_sqd/t))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again random actions and the Q-function for learning\n",
    "act = lambda state : env.action_space.sample()\n",
    "train = lambda s,a,r,ns,d : Q.train(s,a,r,ns,d)\n",
    "\n",
    "alphas = [1. * 10**(-t) for t in range(-1,12)]\n",
    "\n",
    "for alpha in alphas:\n",
    "    # step up Q-function\n",
    "    Q = sc.Q_Learn(lr=alpha)\n",
    "    act_training = lambda state : env.action_space.sample()\n",
    "    train = lambda s,a,r,ns,d : Q.train(s,a,r,ns,d)\n",
    "\n",
    "    for _ in tqdm(iters):\n",
    "        episode(env,act_training,train)\n",
    "    \n",
    "    act_testing = lambda state : Q.act(state)\n",
    "    \n",
    "    mean_reward = 0.\n",
    "    for t in tqdm(iters):\n",
    "        reward = episode(env,act_testing,train)\n",
    "        mean += (reward-mean)/(t+1)\n",
    "        mean_sqd += (reward**2 - mean_sqd)/(t+1)\n",
    "        \n",
    "    print(alpha,mean_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blackjack_print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bandit Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Step-Look-Ahead Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
