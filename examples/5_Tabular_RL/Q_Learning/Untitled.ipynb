{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "class Q_function(defaultdict):\n",
    "\n",
    "    def __init__(self,states=None,actions=None):\n",
    "        # get a dict inside each dictionary entry\n",
    "        super(Q_function, self).__init__(dict)\n",
    "        self.count = defaultdict(lambda : defaultdict())        \n",
    "        ''' \n",
    "        returns the max / argmax / min / argmin value for a state \n",
    "        '''      \n",
    "        \n",
    "        # add preset states and actions\n",
    "        if states is not None :\n",
    "            for state in states :\n",
    "                if actions is None :\n",
    "                    self.add(state)\n",
    "                else :\n",
    "                    for action in actions :\n",
    "                        self.add(state,action)\n",
    "        \n",
    "        if actions is not None :\n",
    "            self.preset_actions = [self._hash(action) for action in actions ]  \n",
    "        else :\n",
    "            self.preset_actions = None\n",
    "\n",
    "        \n",
    "    def max(self,state):\n",
    "        state = self._hash(state)\n",
    "        not_empty = bool(self[state])\n",
    "        return max(self[state].values()) if not_empty else 0.\n",
    "\n",
    "    \n",
    "    def argmax(self,state):\n",
    "        state = self._hash(state)\n",
    "        not_empty = bool(self[state])\n",
    "        if not_empty :\n",
    "            return max(self[state], key=self[state].get)     \n",
    "\n",
    "    def add(self,state,action=None,value=None):  \n",
    "        ''' \n",
    "        add new states, actions, value \n",
    "        returns: hashed state and action\n",
    "        '''\n",
    "        state = self._hash(state)\n",
    "        action = self._hash(action) if action is not None else None\n",
    "\n",
    "        if state not in self.keys():\n",
    "            self[state]\n",
    "            if self.preset_actions is not None:\n",
    "                self[state][action] = 0.\n",
    "        if action is not None and action not in self[state].keys() :\n",
    "            self[state][action] = 0.\n",
    "            self.count[state][action] = 1\n",
    "        if value is not None :\n",
    "            self[state][action] = value\n",
    "        \n",
    "        return (state, action) if action is not None else state\n",
    "  \n",
    "\n",
    "    \n",
    "\n",
    "class Q_learning(Q_function):\n",
    "    '''\n",
    "    Summary: A Q_function with a Q-learning update\n",
    "    \n",
    "    works for maximizing rewards (currently)\n",
    "    \n",
    "    E.g. \n",
    "    Q.learn(state,action,reward,next_state)\n",
    "    \n",
    "    if no next_state then default exit state is None type\n",
    "    '''\n",
    "    def __init__(self,lr=0.1):\n",
    "        self.lr = lr\n",
    "        super(Q_learning, self).__init__()\n",
    "        \n",
    "\n",
    "    def learn(self,state,\\\n",
    "                  action,\\\n",
    "                  reward,\\\n",
    "                  next_state,\\\n",
    "                  done = False,\\\n",
    "                  discount=1.):   \n",
    "        '''\n",
    "        Q-learning update\n",
    "        '''\n",
    "        # add variables and hash where needed\n",
    "        state, action = self.add(state,action)\n",
    "        next_state = self.add(next_state)\n",
    "        \n",
    "        self.count[state][action] +=1\n",
    "        \n",
    "        # the direction of change\n",
    "        \n",
    "        dQ = reward \\\n",
    "            + discount * self.max(next_state) \\\n",
    "            - self[state][action]   \n",
    "            \n",
    "        # correct dQ if is end of episode\n",
    "        if done or next_state is None:\n",
    "            dQ = reward - self[state][action] \n",
    "    \n",
    "\n",
    "            \n",
    "                                     \n",
    "        # The main Q-learning step\n",
    "        self[state][action] = self[state][action] + self.lr * ( dQ )       \n",
    "        \n",
    "    def action(self,state,explore_prob=0.,actions=None):\n",
    "        '''\n",
    "        returns policy action for input state\n",
    "        \n",
    "        can randomize with explore_prob variable\n",
    "        \n",
    "        can add suggest list of actions (in case not seen before)\n",
    "        '''\n",
    "        \n",
    "        if actions is not None :\n",
    "            for act in actions :\n",
    "                self.add(state,act)\n",
    "        \n",
    "        \n",
    "        if random.random() > explore_prob :\n",
    "            return self.argmax(state)\n",
    "        else :\n",
    "            random_action = random.choice(list(self[state].keys()))\n",
    "            return random_action\n",
    "    \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
