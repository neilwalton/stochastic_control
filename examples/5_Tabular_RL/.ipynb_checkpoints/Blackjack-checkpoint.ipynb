{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n",
    "\n",
    "# Blackjack\n",
    "\n",
    "We apply Q-learning to game of Blackjack from the OpenAI gym package.  \n",
    "\n",
    "**Blackjack Rules:**  \n",
    "the rules of the card game Blackjack:\n",
    "- You are dealt two cards (face up). The dealer is dealt two cards (one face up, one face down)\n",
    "- You can ask to get one more card (a hit) which gives you one more card face up or you can stop (stick).\n",
    "- Number cards are worth their face value. King, Queen, Jack are 10. Ace is 1 or 11 (you can choose which).\n",
    "- If your total goes greater then 21 you loose (reward = -1).\n",
    "- If you stick with 21 or below, the dealer then tries to beat or equal your score. \n",
    "- If the dealer gets a score better or equal to you, then you loose (reward = -1).\n",
    "- If the dealer does worst or goes above 21, then you win (reward = +1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning\n",
    "The basic Q-learning update is\n",
    "$$\n",
    "Q(x,a)\n",
    "\\leftarrow\n",
    "Q(x,a)+ \\alpha \\left( r + \\beta \\max_{\\hat a} Q(\\hat x, \\hat a) - Q(x,a)  \\right) \n",
    "$$\n",
    "for (state,action,reward, next state) given by $(x,a,r,\\hat x)$.  \n",
    "(If there is no next state then the maximization above is zero)\n",
    "\n",
    "\n",
    "### Import everything\n",
    "and step up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add location to import stochastic_control\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "# Standard Libraries\n",
    "import numpy as np\n",
    "\n",
    "# Housekeeping modules and settings\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "from IPython.display import clear_output\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "# The main modules that we need\n",
    "import gym\n",
    "import stochastic_control as sc\n",
    "\n",
    "env = gym.make('Blackjack-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def episode(env,act,train=False):\n",
    "    '''Train for one simulation episode\n",
    "        \n",
    "    #Arguments:\n",
    "        env   -- gym enviroment\n",
    "        train -- function : train(state,action,reward,next_state,done)\n",
    "        act   -- function : act(state)\n",
    "        \n",
    "    #Returns: \n",
    "        Cummulative Episode Reward\n",
    "    '''\n",
    "    state = env.reset()\n",
    "    next_state = None\n",
    "    done = False\n",
    "    Reward = 0.\n",
    "    \n",
    "    while done is False:\n",
    "        action = act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        if train is not False:\n",
    "            train(state,action,reward,next_state,done)\n",
    "        state = next_state\n",
    "        Reward += reward\n",
    "    \n",
    "    return Reward\n",
    "    \n",
    "\n",
    "def test(act,iters,verbose=False):\n",
    "    ''' mean and deviation over a number of iterations \n",
    "    \n",
    "    #Arguments:\n",
    "        act - python function \n",
    "        iters - number of iterations\n",
    "        verbose - use tqdm\n",
    "        \n",
    "    #Returns:\n",
    "        mean_reward, standard_deviation/square_root_number_of_iterations\n",
    "    '''\n",
    "    mean = 0.\n",
    "    mean_sqd = 0.\n",
    "    \n",
    "    if verbose :\n",
    "        for t in tqdm(range(iters)):\n",
    "            reward = episode(env,act)\n",
    "            mean += (reward-mean)/(t+1)\n",
    "            mean_sqd += (reward**2 - mean_sqd)/(t+1)            \n",
    "    else: \n",
    "        for t in (range(iters)):\n",
    "            reward = episode(env,act)\n",
    "            mean += (reward-mean)/(t+1)\n",
    "            mean_sqd += (reward**2 - mean_sqd)/(t+1)    \n",
    "        \n",
    "    return mean, np.sqrt((mean_sqd-mean**2)/iters)\n",
    "\n",
    "def blackjack_print(Q):\n",
    "    '''helper function to print nicely'''\n",
    "    \n",
    "    #print table for no usable ace\n",
    "    dealer = range(1,11)\n",
    "    player_false_ace = range(4,22)\n",
    "    print('WITH NO ACE')\n",
    "    print('player\\dealer', end='\\n\\t')\n",
    "    for dlr in dealer:\n",
    "        print(dlr, end = ' ')\n",
    "    print('\\n')\n",
    "\n",
    "    for plyr in player_false_ace:\n",
    "        print(plyr, end='\\t')\n",
    "        for dlr in dealer:\n",
    "            print(Q.argmax((plyr,dlr,False)), end = ' ')\n",
    "        print('')\n",
    "    print('\\n')\n",
    "    \n",
    "    #print table for no usable ace\n",
    "    player_true_ace = range(12,22)\n",
    "    print('WITH ACE')\n",
    "    print('player\\dealer', end='\\n\\t')\n",
    "    for dlr in dealer:\n",
    "        print(dlr, end = ' ')\n",
    "    print('\\n')\n",
    "\n",
    "    for plyr in player_true_ace:\n",
    "        print(plyr, end='\\t')\n",
    "        for dlr in dealer:\n",
    "            print(Q.argmax((plyr,dlr,True)), end = ' ')\n",
    "        print(\"\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Basic Working Example\n",
    "We set a fixed learning rate (0.0001)  \n",
    "We set chose actions (hit/stick) uniformly at random  \n",
    "We run for 20 million episodes (takes a few minutes -- yawn!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Q-function and define action and training functions\n",
    "Q = sc.Q_Learn(lr=0.0001)\n",
    "act = lambda state : env.action_space.sample()\n",
    "train = lambda s,a,r,ns,d : Q.train(s,a,r,ns,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main loop\n",
    "iterations = range(20000000)\n",
    "tic = time()\n",
    "for _ in tqdm(iterations):\n",
    "    episode(env,act,train)\n",
    "toc = time()\n",
    "clear_output(wait=True)\n",
    "print('time=',toc-tic)\n",
    "blackjack_print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test the policy\n",
    "Q_opt = Q\n",
    "iterations = range(10000000)\n",
    "act = lambda s : Q_opt.act(s,actions=[0,1])\n",
    "\n",
    "test(act,10000000,verbose=True)\n",
    "    \n",
    "print('mean score:\\t',mean,' +/- ',np.round(np.sqrt((mean_sqd-mean**2)/t),5))\n",
    "#print('std dev score:\\t',np.round(np.sqrt((mean_sqd-mean**2)/t),5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating Performance\n",
    "We want to investigate the Performance of Q-learning and find good parameters.  \n",
    "\n",
    "Parameters we need to consider are:\n",
    "- How long to train for\n",
    "- Learning rate\n",
    "- exploration probability\n",
    "- reducing the exploration probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching Parameters one-by-one\n",
    "**Training time** Let's fix a good time budget for each training run. Eg. between 1 to 2 seconds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A good number of iterations is:  16384\n"
     ]
    }
   ],
   "source": [
    "# again random actions and the Q-function for learning\n",
    "Q = sc.Q_Learn(lr=0.01)\n",
    "act = lambda state : env.action_space.sample()\n",
    "train = lambda s,a,r,ns,d : Q.train(s,a,r,ns,d)\n",
    "\n",
    "max_run_time = 10. \n",
    "current_run_time = 0.\n",
    "iterations = 1 \n",
    "\n",
    "while current_run_time < 1. :\n",
    "    iterations *= 2 \n",
    "    tic = time()\n",
    "    for _ in tqdm(range(iterations)):\n",
    "        episode(env,act,train)\n",
    "    toc = time()\n",
    "    current_run_time = toc-tic\n",
    "clear_output(wait=True)\n",
    "print('A good number of iterations is: ',iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay so lets round down and take 100,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = 30000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning rate**: Let's find a good learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0 -0.38489 +/- 0.002851\n",
      "1.0 -0.17861 +/- 0.003002\n",
      "0.1 -0.08368 +/- 0.003011\n",
      "0.01 -0.05321 +/- 0.003009\n",
      "best learning rate is : 0.01\n"
     ]
    }
   ],
   "source": [
    "alphas = [1. * 10**(-t) for t in range(-1,3)]\n",
    "Qs ={ alpha : sc.Q_Learn(lr=alpha) for alpha in alphas} \n",
    "\n",
    "def find_alpha(Q_dict,alpha_dict,iters):\n",
    "    ''' Finds the best learning rate\n",
    "    '''\n",
    "    max_mean = -1*np.inf\n",
    "    max_alpha = alphas[0]\n",
    "\n",
    "    for alpha in alpha_dict:\n",
    "        # step up Q-function\n",
    "        act_training = lambda state : env.action_space.sample()\n",
    "        train = lambda s,a,r,ns,d : Q_dict[alpha].train(s,a,r,ns,d,lr=alpha)\n",
    "\n",
    "        for _ in (range(iters)):\n",
    "            episode(env,act_training,train)\n",
    "\n",
    "        act_testing = lambda state : Q_dict[alpha].act(state)\n",
    "        mean, std = test(act_testing,iters)\n",
    "        print(np.round(alpha,6),np.round(mean,6),'+/-',np.round(std,6))\n",
    "\n",
    "        if mean > max_mean :       \n",
    "            max_mean = mean §\n",
    "            max_alpha = alpha\n",
    "    \n",
    "    return max_alpha, max_mean\n",
    "\n",
    "alpha_star, _ = find_alpha(Qs,alphas,iters)\n",
    "print('best learning rate is :',alpha_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot their performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rplt= sc.rolling_plot(5)\n",
    "\n",
    "alphas = [1. * 10**(-t) for t in range(-1,3)]\n",
    "Qs ={ alpha : sc.Q_Learn(lr=alpha) for alpha in alphas} \n",
    "\n",
    "for "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now Repeat:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [03:29<00:00, 21.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha star =  1.0000000000000002e-10 , mean = -0.047580000000000296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "repetitions = 10\n",
    "iters = 50000\n",
    "\n",
    "alpha_star = 1.\n",
    "Q_star = sc.Q_Learn()\n",
    "\n",
    "def Q_alph_update(Q_star,alpha_star,iters):\n",
    "    print('---finding next alpha---')\n",
    "    alphas = [alpha_star * 10**(-t) for t in range(-1,3)]\n",
    "    Qs = { alp : Q_star.copy() for alp in alphas }\n",
    "\n",
    "    alpha_star, score = find_alpha(Qs,alphas,iters)\n",
    "    Q_star = Qs[alpha_star]\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    print('alpha star = ',alpha_star, ', mean =', score)\n",
    "    return Q_star, alpha_star, score\n",
    "\n",
    "Best_Qs = []\n",
    "for _ in tqdm(range(repetitions)):\n",
    "    Q_star, alpha_star, score = Q_alph_update(Q_star,alpha_star,iters)\n",
    "    Best_Qs.append(Q_star.copy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WITH NO ACE\n",
      "player\\dealer\n",
      "\t1 2 3 4 5 6 7 8 9 10 \n",
      "\n",
      "4\t1 1 1 0 1 1 1 1 1 1 \n",
      "5\t1 1 0 0 0 1 1 1 1 1 \n",
      "6\t1 1 1 1 1 1 1 1 1 1 \n",
      "7\t1 1 1 1 1 1 1 1 1 1 \n",
      "8\t1 1 1 1 1 1 1 1 1 1 \n",
      "9\t1 1 1 1 1 1 1 1 1 1 \n",
      "10\t1 1 1 1 1 1 1 1 1 1 \n",
      "11\t1 1 1 1 1 1 1 1 1 1 \n",
      "12\t1 1 0 1 0 0 1 1 1 1 \n",
      "13\t1 0 0 0 0 0 1 1 1 1 \n",
      "14\t1 0 0 0 0 0 1 1 0 1 \n",
      "15\t1 0 0 0 0 0 1 1 1 1 \n",
      "16\t1 0 0 0 0 0 1 0 1 0 \n",
      "17\t1 0 0 0 0 0 0 0 0 0 \n",
      "18\t0 0 0 0 0 0 0 0 0 0 \n",
      "19\t0 0 0 0 0 0 0 0 0 0 \n",
      "20\t0 0 0 0 0 0 0 0 0 0 \n",
      "21\t0 0 0 0 0 0 0 0 0 0 \n",
      "\n",
      "\n",
      "WITH ACE\n",
      "player\\dealer\n",
      "\t1 2 3 4 5 6 7 8 9 10 \n",
      "\n",
      "12\t1 1 1 1 1 1 1 1 1 1 \n",
      "13\t1 1 1 1 1 1 1 1 1 1 \n",
      "14\t1 1 1 1 1 1 1 1 1 1 \n",
      "15\t1 1 1 1 1 1 1 1 1 1 \n",
      "16\t1 1 1 1 1 1 1 1 1 1 \n",
      "17\t1 0 0 0 0 1 0 1 1 1 \n",
      "18\t1 0 0 0 0 0 0 1 0 1 \n",
      "19\t0 0 0 0 0 0 0 0 0 0 \n",
      "20\t0 0 0 0 0 0 0 0 0 0 \n",
      "21\t0 0 0 0 0 0 0 0 0 0 \n"
     ]
    }
   ],
   "source": [
    "blackjack_print(Best_Qs[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exploration**: Now we investigate exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 -0.088933 +/- 0.005486\n",
      "0.2 -0.089633 +/- 0.005524\n",
      "0.30000000000000004 -0.097567 +/- 0.005505\n",
      "0.4 -0.0743 +/- 0.005518\n",
      "0.5 -0.082367 +/- 0.005498\n",
      "0.6 -0.061833 +/- 0.005506\n",
      "0.7000000000000001 -0.060467 +/- 0.005509\n",
      "0.8 -0.0596 +/- 0.005525\n",
      "0.9 -0.0628 +/- 0.005498\n",
      "best exploration prob is : 0.8\n"
     ]
    }
   ],
   "source": [
    "iters = 30000\n",
    "epsilons = np.arange(0.1,1,0.1)\n",
    "\n",
    "# set up Q-functions actions for training and testing and training function\n",
    "Qs = dict()\n",
    "for eps in epsilons:\n",
    "    Qs[eps] = sc.Q_Learn(lr=alpha_star)\n",
    "\n",
    "def find_epsilon(Q_dict,eps_dict,iters):\n",
    "    act_train = dict()\n",
    "    act_test = dict()\n",
    "    trains = dict()\n",
    "    for eps in eps_dict:\n",
    "        act_train[eps] = lambda state : Q_dict[eps].act(state,explore=eps,actions=[0,1])\n",
    "        act_test[eps] = lambda state : Q_dict[eps].act(state,actions=[0,1])\n",
    "        trains[eps] = lambda s,a,r,ns,d : Q_dict[eps].train(s,a,r,ns,d)\n",
    "\n",
    "    max_mean = -1*np.inf\n",
    "    max_eps = epsilons[0]\n",
    "\n",
    "    for eps in eps_dict:\n",
    "\n",
    "        for _ in (range(iters)):\n",
    "            episode(env,act_train[eps],trains[eps])\n",
    "\n",
    "        mean, std = test(act_test[eps],iters)\n",
    "        print(eps,np.round(mean,6),'+/-',np.round(std,6)) \n",
    "\n",
    "        if mean > max_mean :       \n",
    "            max_mean = mean \n",
    "            max_eps = eps\n",
    "    \n",
    "    return max_eps, max_mean\n",
    "    \n",
    "eps_star, _ = find_epsilon(Qs,epsilons,iters)\n",
    "print('best exploration prob is :',eps_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now repeat:** Make sure you keep track of the best solutions found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [14:46<00:00, 87.77s/it]\n"
     ]
    }
   ],
   "source": [
    "repetitions = 10\n",
    "iters = 100000\n",
    "\n",
    "alpha_star = 1.\n",
    "epsilon_star = 1.\n",
    "Q_star = sc.Q_Learn()\n",
    "\n",
    "def Q_alph_update(Q_star,alpha_star,iters):\n",
    "    print('---finding alpha---')\n",
    "    alphas = [alpha_star * 10**(-t) for t in range(-1,3)]\n",
    "    Qs = { alp : Q_star.copy() for alp in alphas }\n",
    "\n",
    "    alpha_star, score = find_alpha(Qs,alphas,iters)\n",
    "    Q_star = Qs[alpha_star]\n",
    "    print('alpha star = ',alpha_star)\n",
    "    \n",
    "    return Q_star, alpha_star, score\n",
    "\n",
    "def Q_eps_update(Q_star,epsilon_star,iters):\n",
    "    print('---finding epsilon---')\n",
    "    epsilons = [epsilon_star * m for m in [0.2,0.4,0.6,0.8,1.]]\n",
    "    Qs = { eps : Q_star.copy() for eps in epsilons }\n",
    "    \n",
    "    epsilon_star, score = find_epsilon(Qs,epsilons,iters)\n",
    "    Q_star = Qs[epsilon_star]\n",
    "    print('epsilon star = ',epsilon_star)\n",
    "    \n",
    "    return Q_star, epsilon_star, score\n",
    "\n",
    "Best_Qs = []\n",
    "for _ in tqdm(range(repetitions)):\n",
    "    Q_star, alpha_star, score = Q_alph_update(Q_star,alpha_star,iters)\n",
    "    Best_Qs.append(Q_star.copy())\n",
    "    Q_star, epsilon_star, score = Q_eps_update(Q_star,epsilon_star,iters)\n",
    "    Best_Qs.append(Q_star.copy())\n",
    "    clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now:** let's evaluate of best solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -0.05539000000000009 +/- 0.0030132027468970563\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'max_mean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-8c8aea3f500d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'+/-'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_sqd\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mmean\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmax_mean\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mmax_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mmax_alpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'max_mean' is not defined"
     ]
    }
   ],
   "source": [
    "for idx, Qs in enumerate(Best_Qs):\n",
    "    act = lambda s : Qs.act(s)\n",
    "    mean = 0.\n",
    "    mean_sqd = 0.\n",
    "    for t in (range(100000)):\n",
    "        reward = episode(env,act)\n",
    "        mean += (reward-mean)/(t+1)\n",
    "        mean_sqd += (reward**2 - mean_sqd)/(t+1)\n",
    "\n",
    "    print(idx,mean,'+/-',np.sqrt(mean_sqd/t)) \n",
    "\n",
    "    if mean > max_mean :       \n",
    "        max_mean = mean \n",
    "        max_alpha = alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blackjack_print(Best_Qs[16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [lambda s,a,r,ns,d : Q_dict[new].train(s,a,r,ns,d) for new in [0,1]]\n",
    "acts =[None,None]\n",
    "\n",
    "means = [0., 0.]\n",
    "means_sqd = [0., 0.]\n",
    "sigma = [0.,0.]\n",
    "\n",
    "tic = time()\n",
    "toc = 0 \n",
    "while toc < 600:\n",
    "    \n",
    "    print('training')\n",
    "    for new in [0,1]:\n",
    "        for _ in (range(iters)):\n",
    "            episode(env,rand_act,train[new])\n",
    "\n",
    "    print('testing')   \n",
    "    for new in [0,1]:\n",
    "        acts[new] = lambda state : Qs[new].act(state,actions=[0,1])\n",
    "        for t in (range(iters)):\n",
    "            reward = episode(env,acts[new],train[new])\n",
    "            means[new] += (reward-means[new])/(t+1)\n",
    "            means_sqd[new] += (reward**2 - means_sqd[new])/(t+1)\n",
    "            \n",
    "        sigma[new] = np.sqrt(means_sqd[new]/(t+1))\n",
    "\n",
    "        print(Qs[new].lr,means[new],'+/-',np.sqrt(means_sqd[new]/(t+1)))     \n",
    "\n",
    "    if means[1]-sigma[1] > means[0]+sigma[0]:\n",
    "        print('update learning rate to', Qs[1].lr)\n",
    "        Qs[0] = Qs[1]\n",
    "        Qs[1] = Qs[1].copy()\n",
    "        Qs[1].lr /= 2\n",
    "        \n",
    "        means = [0., 0.]\n",
    "        means_sqd = [0., 0.]\n",
    "    elif means[0]-sigma[0] > means[1]+sigma[1] :\n",
    "        print('keep current rate')\n",
    "        Qs[1] = Qs[0].copy()\n",
    "        \n",
    "    else:\n",
    "        iters = int(1.1*iters)\n",
    "        print('i training to ',iters)\n",
    "    \n",
    "    toc = time()-tic\n",
    "    print('time= ',toc)\n",
    "        \n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blackjack_print(Qs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = 1000\n",
    "alphas = [1., .5]\n",
    "lr_factor = 4\n",
    "Qs = [sc.Q_Learn(1.), sc.Q_Learn(.5)]\n",
    "rand_act = lambda state : env.action_space.sample()\n",
    "train = [lambda s,a,r,ns,d : Qs[new].train(s,a,r,ns,d) for new in [0,1]]\n",
    "acts =[None,None]\n",
    "\n",
    "means = [0., 0.]\n",
    "means_sqd = [0., 0.]\n",
    "sigma = [0.,0.]\n",
    "N = [0, 0]\n",
    "\n",
    "tic = time()\n",
    "toc = 0 \n",
    "\n",
    "while toc < 60*10:\n",
    "    \n",
    "\n",
    "    for new in [0,1]:\n",
    "        acts[new] = lambda state : Qs[new].act(state,actions=[0,1])\n",
    "        for _ in (range(iters)):\n",
    "            episode(env,rand_act,train[new])   \n",
    "            reward = episode(env,acts[new],train[new])\n",
    "            means[new] += (reward-means[new])/(N[new]+1)\n",
    "            means_sqd[new] += (reward**2 - means_sqd[new])/(N[new]+1)\n",
    "            sigma[new] = np.sqrt(means_sqd[new]/(N[new]+1))\n",
    "            N[new]+= 1\n",
    "\n",
    "        print(Qs[new].lr,means[new],'+/-',np.sqrt(means_sqd[new]/(N[new]+1)))     \n",
    "\n",
    "    if means[1]-sigma[1] > means[0]+sigma[0]:\n",
    "        print('update learning rate to', Qs[1].lr)\n",
    "        \n",
    "        Qs[0] = Qs[1]\n",
    "        Qs[1] = Qs[1].copy()\n",
    "        \n",
    "        Qs[1].lr /= lr_factor\n",
    "        \n",
    "        N[0] = N[1]\n",
    "        N[1] = 0\n",
    "        \n",
    "        means[0] = means[1]\n",
    "        means_sqd[0] = means_sqd[1]\n",
    "\n",
    "        means[1] = 0.\n",
    "        means_sqd[1] = 0.\n",
    "\n",
    "    elif means[0]-sigma[0] > means[1]+sigma[1] :\n",
    "        print('keep current rate')\n",
    "        \n",
    "        Qs[1] = Qs[0].copy()\n",
    "        Qs[1].lr = Qs[0].lr/ lr_factor\n",
    "        \n",
    "        N[1] = 0\n",
    "\n",
    "    else:\n",
    "        iters = int(2*iters)\n",
    "        print('update iterations training to ',iters)\n",
    "\n",
    "    toc = time()-tic\n",
    "    #clear_output(wait=True)\n",
    "    print('-----------------')\n",
    "    print('time = ',toc)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Countinous update\n",
    "'''\n",
    "\n",
    "iters = 100\n",
    "alphas = [1., .1]\n",
    "lr_factor = 10\n",
    "Qs = [sc.Q_Learn(1.), sc.Q_Learn(.1)]\n",
    "rand_act = lambda state : env.action_space.sample()\n",
    "train = [lambda s,a,r,ns,d : Qs[new].train(s,a,r,ns,d) for new in [0,1]]\n",
    "acts =[None,None]\n",
    "\n",
    "means = [0., 0.]\n",
    "means_sqd = [0., 0.]\n",
    "sigma = [0.,0.]\n",
    "N = [0, 0]\n",
    "\n",
    "\n",
    "tic = time()\n",
    "toc = 0 \n",
    "it = 0\n",
    "\n",
    "while toc < 360:    \n",
    "    it += 1\n",
    "    \n",
    "    if it % 100 == 0 :\n",
    "        print(it/100000, np.round(time()-tic,0))\n",
    "        for new in [0,1]:\n",
    "            print(np.round(Qs[new§].lr,6),\n",
    "                  np.round(means[new],6),\n",
    "                  '+/-',np.round(np.sqrt(means_sqd[new]/(N[new]+1)),6))  \n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    for new in [0,1]:\n",
    "        acts[new] = lambda state : Qs[new].act(state,actions=[0,1])\n",
    "\n",
    "        episode(env,rand_act,train[new])   \n",
    "        reward = episode(env,acts[new],train[new])\n",
    "        means[new] += (reward-means[new])/(N[new]+1)\n",
    "        means_sqd[new] += (reward**2 - means_sqd[new])/(N[new]+1)\n",
    "        sigma[new] = np.sqrt(means_sqd[new]/(N[new]+1))\n",
    "        N[new]+= 1\n",
    "\n",
    "        if means[1]-2*sigma[1] > means[0]+ 2*sigma[0]:\n",
    "\n",
    "            Qs[0] = Qs[1]\n",
    "            Qs[1] = Qs[1].copy()\n",
    "\n",
    "            Qs[1].lr /= lr_factor\n",
    "\n",
    "            N[0] = N[1]\n",
    "            N[1] = 0\n",
    "\n",
    "            means[0] = means[1]\n",
    "            means_sqd[0] = means_sqd[1]\n",
    "\n",
    "            means[1] = means[1]\n",
    "            means_sqd[1] = 100.\n",
    "\n",
    "    toc = time()-tic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blackjack_print(Qs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "alphas = [1. * 10**(-t) for t in range(-1,9)]\n",
    "Q_dict = dict()\n",
    "for alpha in alphas:\n",
    "    Q_dict[alpha] = sc.Q_Learn(lr=alpha)\n",
    "\n",
    "for alpha in alphas:\n",
    "    # step up Q-function\n",
    "    act_training = lambda state : env.action_space.sample()\n",
    "    train = lambda s,a,r,ns,d : Q_dict[alpha].train(s,a,r,ns,d)\n",
    "\n",
    "    for _ in tqdm(iters):\n",
    "        episode(env,train,act_training)\n",
    "    \n",
    "    act_testing = lambda state : Q_dict[alpha].act(state)\n",
    "    \n",
    "    mean = 0.\n",
    "    mean_sqd = 0.\n",
    "    for t in tqdm(iters):\n",
    "        reward = episode(env,train,act_testing)\n",
    "        mean += (reward-mean)/(t+1)\n",
    "        mean_sqd += (reward**2 - mean_sqd)/(t+1)\n",
    "        \n",
    "    print(alpha,mean,'+/-',np.sqrt(mean_sqd/t))   \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "epss = [1., .5 , 0.1, 0.05, 0.01]\n",
    "\n",
    "Q_dict = dict()\n",
    "for eps in epss:\n",
    "    Q_dict[eps] = sc.Q_Learn(lr=alpha)\n",
    "\n",
    "for eps in epss:\n",
    "    act_training = lambda state :  Q_dict[eps].act(state,eps,actions=[0,1])\n",
    "    train = lambda s,a,r,ns,d : Q_dict[eps].train(s,a,r,ns,d)\n",
    "\n",
    "    for _ in tqdm(iters):\n",
    "        episode(env,act_training,train)\n",
    "\n",
    "    act_testing = lambda state : Q_dict[eps].act(state)\n",
    "\n",
    "    mean = 0.\n",
    "    mean_sqd = 0.\n",
    "    for t in tqdm(iters):\n",
    "        reward = episode(env,act_testing,train)\n",
    "        mean += (reward-mean)/(t+1)\n",
    "        mean_sqd += (reward**2 - mean_sqd)/(t+1)\n",
    "\n",
    "    print(eps,mean,'+/-',np.sqrt(mean_sqd/t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = range(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [1. * 10**(-t) for t in range(-1,9)]\n",
    "Q_dict_old = Q_dict\n",
    "Q_dict = dict()\n",
    "for alpha in alphas:\n",
    "    Q_dict[alpha] = Q_dict_old[1.].copy()\n",
    "\n",
    "for alpha in alphas:\n",
    "    # step up Q-function\n",
    "    act_training = lambda state : env.action_space.sample()\n",
    "    train = lambda s,a,r,ns,d : Q_dict[alpha].train(s,a,r,ns,d)\n",
    "\n",
    "    for _ in tqdm(iters):\n",
    "        episode(env,act_training,train)\n",
    "    \n",
    "    act_testing = lambda state : Q_dict[alpha].act(state)\n",
    "    \n",
    "    mean = 0.\n",
    "    mean_sqd = 0.\n",
    "    for t in tqdm(iters):\n",
    "        reward = episode(env,act_testing,train)\n",
    "        mean += (reward-mean)/(t+1)\n",
    "        mean_sqd += (reward**2 - mean_sqd)/(t+1)\n",
    "        \n",
    "    print(alpha,mean,'+/-',np.sqrt(mean_sqd/t))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again random actions and the Q-function for learning\n",
    "act = lambda state : env.action_space.sample()\n",
    "train = lambda s,a,r,ns,d : Q.train(s,a,r,ns,d)\n",
    "\n",
    "alphas = [1. * 10**(-t) for t in range(-1,12)]\n",
    "\n",
    "for alpha in alphas:\n",
    "    # step up Q-function\n",
    "    Q = sc.Q_Learn(lr=alpha)\n",
    "    act_training = lambda state : env.action_space.sample()\n",
    "    train = lambda s,a,r,ns,d : Q.train(s,a,r,ns,d)\n",
    "\n",
    "    for _ in tqdm(iters):\n",
    "        episode(env,act_training,train)\n",
    "    \n",
    "    act_testing = lambda state : Q.act(state)\n",
    "    \n",
    "    mean_reward = 0.\n",
    "    for t in tqdm(iters):\n",
    "        reward = episode(env,act_testing,train)\n",
    "        mean += (reward-mean)/(t+1)\n",
    "        mean_sqd += (reward**2 - mean_sqd)/(t+1)\n",
    "        \n",
    "    print(alpha,mean_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blackjack_print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bandit Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Step-Look-Ahead Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
