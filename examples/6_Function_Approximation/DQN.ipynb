{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "antique-immigration",
   "metadata": {},
   "source": [
    "# DQN implementation\n",
    "\n",
    "https://www.datascienceassn.org/sites/default/files/Human-level%20Control%20Through%20Deep%20Reinforcement%20Learning.pdf\n",
    "\n",
    "useful sites\n",
    "\n",
    "https://github.com/openai/baselines/tree/master/baselines\n",
    "https://github.com/DavidJanz/successor_uncertainties_atari\n",
    "https://github.com/davidreiman/pytorch-atari-dqn\n",
    "https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "\n",
    "https://github.com/roclark/openai-gym-pytorch\n",
    "\n",
    "cartpole\n",
    "\n",
    "https://github.com/Rowing0914/TF_RL/blob/master/tf_rl/env/cartpole_pixel.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "varying-reputation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random\n",
    "import torchvision\n",
    "import gym\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "adolescent-israel",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../../stochastic_control/neural_rl/')\n",
    "\n",
    "import numpy as np\n",
    "import stochastic_control\n",
    "from atari_env_torch import make_atari\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "comparable-balloon",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dying-operator",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_Network(nn.Module):\n",
    "    ''' A Deepmind-type DQN network\n",
    "    '''  \n",
    "    def __init__(self,action_size=6):\n",
    "        super(DQN_Network, self).__init__()\n",
    "        self._args = (action_size,)\n",
    "        self.dqn_model = nn.Sequential(\n",
    "             nn.Conv2d(4, 32, 8, 4),\n",
    "             nn.ReLU(),\n",
    "             nn.Conv2d(32, 64, 4, 2),\n",
    "             nn.ReLU(),\n",
    "             nn.Conv2d(64, 64, 3, 1),\n",
    "             nn.ReLU(),\n",
    "             nn.Flatten(),\n",
    "             nn.Linear(3136,1024),\n",
    "             nn.ReLU(),\n",
    "             nn.Linear(1024, action_size) )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.dqn_model(x)\n",
    "        return output\n",
    "    \n",
    "    def clone(self):# Should this be here?\n",
    "        clone = DQN_Network(*self._args) # Check this? Change self?\n",
    "        clone.load_state_dict(self.state_dict())\n",
    "        return clone   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "stretch-seven",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' to do:\n",
    "\n",
    "Need to add target parameters etc.. to DQN below\n",
    "epsilon decay\n",
    "check everything\n",
    "'''\n",
    "\n",
    "class DQN(): \n",
    "    def __init__(self,\n",
    "                 discount,\n",
    "                 action_size,\n",
    "                 lr,\n",
    "                 neural_network,\n",
    "                 batch_size,\n",
    "                 memory_size,\n",
    "                 target_steps,\n",
    "                 epsilon_steps,\n",
    "                 epsilon_final,\n",
    "                 epsilon_start=1.):\n",
    "        \n",
    "        # MDP parameters\n",
    "        self.disc = discount                       \n",
    "        self.action_size = action_size        \n",
    "\n",
    "        # Network optimization\n",
    "        self.q_fn = neural_network\n",
    "        self.lr = lr                              \n",
    "        self.q_fn_target = self.q_fn.clone()\n",
    "        self.optimizer = Adam(self.q_fn.parameters(), lr=self.lr)   \n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Training parameters\n",
    "        self.memory = deque(maxlen=memory_size)             \n",
    "        self.eps = epsilon_start\n",
    "        self.eps_final = epsilon_final      \n",
    "        self.epsilon_steps = epsilon_steps\n",
    "        self.target_steps = target_steps\n",
    "        \n",
    "        # Internal parameters\n",
    "        self.target_counter = 0       \n",
    "        self.memory_full = False\n",
    "        \n",
    "    def update(self,sarsd):\n",
    "        # 1. memorize sarsd\n",
    "        self.memorize(sarsd)\n",
    "        \n",
    "        # 2. learn a batch (Can this be shorter!?)\n",
    "        if len(self.memory) == memory_size:\n",
    "            batch = self.batch(self.batch_size)\n",
    "            self.learn(batch)\n",
    "            if self.memory_full is not True:\n",
    "                self.memory_full = True\n",
    "                print('Memory now full!')\n",
    "        \n",
    "        # 3. update counter and epsilon (once memory full)\n",
    "        s, a, r, s, d = sarsd\n",
    "        self.target_counter += 1\n",
    "        if self.memory_full:\n",
    "            self._eps_update()\n",
    "                  \n",
    "        # 4. update target if update required   \n",
    "        if self.target_counter > self.target_steps:\n",
    "            self.target_counter = 0\n",
    "            self._target_update()\n",
    "               \n",
    "    def policy(self, state, epsilon=None):# checked\n",
    "        eps = self.eps if epsilon is None else epsilon      \n",
    "        if np.random.rand() < eps:\n",
    "            action = np.random.randint(self.action_size)\n",
    "        else:\n",
    "            action = self.q_fn(state).max(1)[1].item()      \n",
    "        return action\n",
    "        \n",
    "    def learn(self,batch):\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.loss(batch)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def save(self):\n",
    "        torch.save({\n",
    "            'q_fn': self.q_fn.state_dict(),\n",
    "            'eps': self.eps\n",
    "            }, './checkpoints/tensor.pt')\n",
    "        \n",
    "    def load(self):\n",
    "        pass\n",
    "    \n",
    "    def memorize(self, sarsd):# checked\n",
    "        self.memory.append(sarsd)\n",
    "        \n",
    "    def batch(self,batch_size):# checked\n",
    "        # returns a batch of states, actions, rewards, next_states, dones\n",
    "        idx=np.random.choice(len(self.memory), batch_size, replace=False)\n",
    "        s_batch = torch.cat([self.memory[i][0] for i in idx])\n",
    "        a_batch = torch.LongTensor([[self.memory[i][1]] for i in idx])\n",
    "        r_batch = torch.FloatTensor([self.memory[i][2] for i in idx])\n",
    "        ns_batch = torch.cat([self.memory[i][3] for i in idx])\n",
    "        d_batch = torch.FloatTensor([self.memory[i][4] for i in idx])\n",
    "        return (s_batch, a_batch, r_batch, ns_batch, d_batch)\n",
    "            \n",
    "    def _eps_update(self):  \n",
    "        self.eps = max(self.eps - 1/self.epsilon_steps,self.eps_final)\n",
    "    \n",
    "    def loss(self,batch):\n",
    "        s, a, r, ns, d = batch\n",
    "        target = r+ (1-d)* self.disc * self.q_fn_target(ns).max(1)[0].detach()\n",
    "        prediction = self.q_fn(s).gather(1,a).squeeze()\n",
    "        return F.smooth_l1_loss(prediction, target)\n",
    "    \n",
    "    def _target_update(self):\n",
    "        self.q_fn_target = self.q_fn.clone()\n",
    "        self.target_counter = 0\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specialized-repair",
   "metadata": {},
   "source": [
    "## Atari Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "material-princess",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_atari('PongNoFrameskip-v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "english-april",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MDP parameters\n",
    "discount = 0.99\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Network optimization\n",
    "dqn_net = DQN_Network()\n",
    "lr = 5e-5\n",
    "batch_size = 32\n",
    "\n",
    "# Training parameters\n",
    "target_steps = int(1e4)\n",
    "memory_size = int(1e6)\n",
    "epsilon_steps = int(1e6) \n",
    "epsilon_final = 0.1\n",
    "\n",
    "total_steps = int(5e7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "western-vocabulary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MDP parameters\n",
    "discount = 0.99\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Network optimization\n",
    "dqn_net = DQN_Network()\n",
    "lr = 5e-5\n",
    "batch_size = 32\n",
    "\n",
    "# Training parameters\n",
    "target_steps = int(1e4)\n",
    "memory_size = int(1e4)\n",
    "epsilon_steps = int(1e5) \n",
    "epsilon_final = 0.05\n",
    "\n",
    "total_steps = int(1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "useful-missile",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_net = DQN_Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "outstanding-amount",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQN(discount,\n",
    "          action_size,\n",
    "          lr,\n",
    "          dqn_net,\n",
    "          batch_size,\n",
    "          memory_size,\n",
    "          target_steps,\n",
    "          epsilon_steps,\n",
    "          epsilon_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "scientific-found",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 984/10000 [00:01<00:12, 717.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory now full!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [13:44<00:00, 12.13it/s]\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "s = env.reset()\n",
    "for _ in tqdm(range(total_steps)):\n",
    "    a = dqn.policy(s)\n",
    "    ns, r, d, _ = env.step(a)\n",
    "    sarsd = (s,a,r,ns,float(d))\n",
    "    dqn.update(sarsd)\n",
    "    if d:\n",
    "        s = env.reset()\n",
    "    else:\n",
    "        s = ns  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documented-proceeding",
   "metadata": {},
   "source": [
    "## Cartpol Easy Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "eight-horror",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "scenic-technology",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPole_Net(nn.Module):\n",
    "    ''' A CartPole network\n",
    "    '''  \n",
    "    def __init__(self,action_size=2):\n",
    "        super(CartPole_Net, self).__init__()\n",
    "        self._args = (action_size,)\n",
    "        self.dqn_model = nn.Sequential(\n",
    "             nn.Linear(4, 32),\n",
    "             nn.ReLU(),\n",
    "             nn.Linear(32, 32),\n",
    "             nn.ReLU(),\n",
    "             nn.Linear(32, 32),\n",
    "             nn.ReLU(),\n",
    "             nn.Linear(32, action_size) )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.dqn_model(x)\n",
    "        return output\n",
    "    \n",
    "    def clone(self):# Should this be here?\n",
    "        clone = CartPole_Net(*self._args) # Check this? Change self?\n",
    "        clone.load_state_dict(self.state_dict())\n",
    "        return clone   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "italic-scout",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cartpole_sarsd(s,a,r,ns,d):\n",
    "    s = torch.from_numpy(s).float().unsqueeze(0)\\\n",
    "    if isinstance(s,np.ndarray) else s\n",
    "    ns = torch.from_numpy(ns).float().unsqueeze(0)\\\n",
    "    if isinstance(ns,np.ndarray) else ns\n",
    "    d = float(d)\n",
    "    r= r-d\n",
    "    return (s,a,r,ns,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ecological-puzzle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MDP parameters\n",
    "discount = 0.99\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Network optimization\n",
    "dqn_net = CartPole_Net()\n",
    "lr = 1e-2\n",
    "batch_size = 32\n",
    "\n",
    "# Training parameters\n",
    "target_steps = int(1e3)\n",
    "memory_size = int(1e4)\n",
    "epsilon_steps = int(1e4) \n",
    "epsilon_final = 0.1\n",
    "\n",
    "total_steps = int(1e4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "moved-cosmetic",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQN(discount,\n",
    "          action_size,\n",
    "          lr,\n",
    "          dqn_net,\n",
    "          batch_size,\n",
    "          memory_size,\n",
    "          target_steps,\n",
    "          epsilon_steps,\n",
    "          epsilon_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "collective-provincial",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 22587.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory now full!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "s = torch.tensor(env.reset()).float().unsqueeze(0)\n",
    "for _ in tqdm(range(total_steps)):\n",
    "    a = dqn.policy(s)\n",
    "    ns, r, d, _ = env.step(a)\n",
    "    sarsd = (s,a,r,ns,d) = cartpole_sarsd(s,a,r,ns,d)\n",
    "    dqn.update(sarsd)\n",
    "    if d:\n",
    "        s = torch.tensor(env.reset()).float().unsqueeze(0)\n",
    "    else:\n",
    "        s = ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "banned-morocco",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "s = torch.tensor(env.reset()).float().unsqueeze(0)\n",
    "d = False\n",
    "while not d:\n",
    "    a = dqn.policy(s)\n",
    "    ns, r, d, _ = env.step(a)\n",
    "    print(r-d)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immediate-leeds",
   "metadata": {},
   "source": [
    "## Learning Rewards and Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "voluntary-efficiency",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_reward(self,batch):\n",
    "    s, a, r, ns, d = batch\n",
    "    target = r-d\n",
    "    prediction = self.q_fn(s).gather(1,a).squeeze()\n",
    "    return F.smooth_l1_loss(prediction, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "macro-investment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MDP parameters\n",
    "discount = 0.99\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Network optimization\n",
    "dqn_net = CartPole_Net()\n",
    "lr = 1e-2\n",
    "batch_size = 32\n",
    "\n",
    "total_steps = int(1e4)\n",
    "# Training parameters\n",
    "target_steps = total_steps\n",
    "memory_size = total_steps\n",
    "epsilon_steps = batch_size\n",
    "epsilon_final = 1.\n",
    "\n",
    "#total_steps = int(1e5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "short-mixer",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQN(discount,\n",
    "          action_size,\n",
    "          lr,\n",
    "          dqn_net,\n",
    "          batch_size,\n",
    "          memory_size,\n",
    "          target_steps,\n",
    "          epsilon_steps,\n",
    "          epsilon_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "guilty-automation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:01<00:00, 9736.91it/s]\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "s = torch.tensor(env.reset()).float().unsqueeze(0)\n",
    "for _ in tqdm(range(total_steps)):\n",
    "    a = dqn.policy(s)\n",
    "    ns, r, d, _ = env.step(a)\n",
    "    sarsd = (s,a,r,ns,d) = cartpole_sarsd(s,a,r,ns,d)\n",
    "    dqn.memorize(sarsd)\n",
    "    if d:\n",
    "        s = torch.tensor(env.reset()).float().unsqueeze(0)\n",
    "    else:\n",
    "        s = ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "dirty-artist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dqn.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "seeing-ireland",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for _ in range(1000):\n",
    "batch = dqn.batch(32)\n",
    "dqn.learn(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "atlantic-newman",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.optimizer.zero_grad()\n",
    "batch = dqn._batch(batch_size)\n",
    "loss = loss_reward(dqn,batch)\n",
    "loss.backward()\n",
    "dqn.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "suitable-elevation",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = dqn.batch(len(dqn.memory))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "major-phase",
   "metadata": {},
   "source": [
    "## you are here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "lovely-norway",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [1],\n",
       "        [0],\n",
       "        ...,\n",
       "        [0],\n",
       "        [1],\n",
       "        [1]])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "vietnamese-product",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "gather() received an invalid combination of arguments - got (int, int), but expected one of:\n * (name dim, Tensor index, *, bool sparse_grad)\n * (int dim, Tensor index, *, bool sparse_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-1cf9f63017f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0msarsd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcartpole_sarsd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msarsd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-54-cf48db9172bc>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-54-cf48db9172bc>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisc\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_fn_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmooth_l1_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: gather() received an invalid combination of arguments - got (int, int), but expected one of:\n * (name dim, Tensor index, *, bool sparse_grad)\n * (int dim, Tensor index, *, bool sparse_grad)\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "s = torch.tensor(env.reset()).float().unsqueeze(0)\n",
    "for _ in tqdm(range(total_steps)):\n",
    "    a = np.random.randint(2)\n",
    "    ns, r, d, _ = env.step(a)\n",
    "    sarsd = (s,a,r,ns,d) = cartpole_sarsd(s,a,r,ns,d)\n",
    "    dqn.learn(sarsd)\n",
    "    if d:\n",
    "        s = torch.tensor(env.reset()).float().unsqueeze(0)\n",
    "    else:\n",
    "        s = ns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legitimate-hopkins",
   "metadata": {},
   "source": [
    "# Debug env "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reverse-membrane",
   "metadata": {},
   "source": [
    "## Debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cloudy-robert",
   "metadata": {},
   "source": [
    "### Define Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driving-crystal",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gym)",
   "language": "python",
   "name": "gym"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
