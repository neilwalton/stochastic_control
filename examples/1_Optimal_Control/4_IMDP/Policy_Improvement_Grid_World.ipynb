{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Improvement on GRID Game Example\n",
    "\n",
    "First lets import standard numeric library\n",
    "<img src=\"./Pictures/Grid.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv, det"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define the costs of the cells in the grid.\n",
    "\n",
    "Here $\\epsilon$ the probability of a random action\n",
    "\n",
    "and $\\beta$ is the decay rate of our reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grid=np.matrix([0,0,0,-1,0,-2,0,0,0,0,0,+2])\n",
    "r = np.transpose(Grid)\n",
    "Epsilon = 0.8\n",
    "beta = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the jumps taken from following each action: Left, Right, Up, Down and a Random Policy.\n",
    "\n",
    "\n",
    "(This could probably be automated for bigger examples.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_Left = np.array(\n",
    "                    [\n",
    "                    [1,0,0,0,0,0,0,0,0,0,0,0],\n",
    "                    [1,0,0,0,0,0,0,0,0,0,0,0],\n",
    "                    [0,1,0,0,0,0,0,0,0,0,0,0],\n",
    "                    [0,0,0,1,0,0,0,0,0,0,0,0],\n",
    "                    [0,0,0,0,1,0,0,0,0,0,0,0],\n",
    "                    [0,0,0,0,0,1,0,0,0,0,0,0],\n",
    "                    [0,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                    [0,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                    [0,0,0,0,0,0,0,0,1,0,0,0],\n",
    "                    [0,0,0,0,0,0,0,0,1,0,0,0],\n",
    "                    [0,0,0,0,0,0,0,0,0,1,0,0],\n",
    "                    [0,0,0,0,0,0,0,0,0,0,0,1]\n",
    "                    ]\n",
    "                   )\n",
    "\n",
    "P_Right = np.array(\n",
    "                    [\n",
    "                    [0,1,0,0,0,0,0,0,0,0,0,0],\n",
    "                    [0,0,1,0,0,0,0,0,0,0,0,0],\n",
    "                    [0,0,0,1,0,0,0,0,0,0,0,0],\n",
    "                    [0,0,0,1,0,0,0,0,0,0,0,0],\n",
    "                    [0,0,0,0,1,0,0,0,0,0,0,0],\n",
    "                    [0,0,0,0,0,1,0,0,0,0,0,0],\n",
    "                    [0,0,0,0,0,0,0,1,0,0,0,0],\n",
    "                    [0,0,0,0,0,0,0,1,0,0,0,0],\n",
    "                    [0,0,0,0,0,0,0,0,0,1,0,0],\n",
    "                    [0,0,0,0,0,0,0,0,0,0,1,0],\n",
    "                    [0,0,0,0,0,0,0,0,0,0,0,1],\n",
    "                    [0,0,0,0,0,0,0,0,0,0,0,1]\n",
    "                    ]\n",
    "                   )\n",
    "                   \n",
    "P_Up = np.array(\n",
    "                    [\n",
    "                    [0,0,0,0,1,0,0,0,0,0,0,0],\n",
    "                    [0,1,0,0,0,0,0,0,0,0,0,0],\n",
    "                    [0,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                    [0,0,0,1,0,0,0,0,0,0,0,0],\n",
    "                    [0,0,0,0,0,0,0,0,1,0,0,0],\n",
    "                    [0,0,0,0,0,1,0,0,0,0,0,0],\n",
    "                    [0,0,0,0,0,0,0,0,0,0,1,0],\n",
    "                    [0,0,0,0,0,0,0,0,0,0,0,1],\n",
    "                    [0,0,0,0,0,0,0,0,1,0,0,0],\n",
    "                    [0,0,0,0,0,0,0,0,0,1,0,0],\n",
    "                    [0,0,0,0,0,0,0,0,0,0,1,0],\n",
    "                    [0,0,0,0,0,0,0,0,0,0,0,1],\n",
    "                    ]\n",
    "                   )\n",
    "                   \n",
    "P_Down = np.array(\n",
    "                    [\n",
    "                    [1,0,0,0,0,0,0,0,0,0,0,0],\n",
    "                    [0,1,0,0,0,0,0,0,0,0,0,0],\n",
    "                    [0,0,1,0,0,0,0,0,0,0,0,0],\n",
    "                    [0,0,0,1,0,0,0,0,0,0,0,0],\n",
    "                    [1,0,0,0,0,0,0,0,0,0,0,0],\n",
    "                    [0,0,0,0,0,1,0,0,0,0,0,0],\n",
    "                    [0,0,1,0,0,0,0,0,0,0,0,0],\n",
    "                    [0,0,0,1,0,0,0,0,0,0,0,0],\n",
    "                    [0,0,0,0,1,0,0,0,0,0,0,0],\n",
    "                    [0,0,0,0,0,1,0,0,0,0,0,0],\n",
    "                    [0,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                    [0,0,0,0,0,0,0,0,0,0,0,1]\n",
    "                    ]\n",
    "                   )\n",
    "\n",
    "P_Random = 0.25*P_Left + 0.25*P_Right + 0.25*P_Up + 0.25*P_Down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the transition probabilities for each action (given that a random action might be taken).\n",
    "\n",
    "We collect these into a big array where:\n",
    "\n",
    "0 = Left   \n",
    "1 = Right   \n",
    "2 = Up   \n",
    "3 = Down   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_Left = ( ( 1 - Epsilon ) * P_Left + Epsilon * P_Random ) \n",
    "Q_Right = ( ( 1 - Epsilon ) * P_Right + Epsilon * P_Random ) \n",
    "Q_Up = ( ( 1 - Epsilon ) * P_Up + Epsilon * P_Random ) \n",
    "Q_Down = ( ( 1 - Epsilon ) * P_Down + Epsilon * P_Random ) \n",
    "\n",
    "P = [Q_Left, Q_Right, Q_Up, Q_Down] # 0 = Left, 1 = Right, 2 = Up, 3 = Down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we get the rewards of a policy $\\pi$.\n",
    "\n",
    "Recall that $\\pi$ defines a Markov chain $X^\\pi_t$ and that we can evaluate \n",
    "   \n",
    "\\begin{equation}\n",
    "R^\\pi(x) = \n",
    "\\mathbb E_x \n",
    "\\Big[\n",
    "\\sum_{t=0}^\\infty \\beta^t r(X_t)\n",
    "\\Big]\n",
    "\\end{equation}  \n",
    "by solving the equation\n",
    "   \n",
    "\\begin{equation}\n",
    "R^\\pi(x) = \\beta (Q^{\\pi} R^{\\pi} )(x) +r(x)\n",
    "\\end{equation}\n",
    "   \n",
    "Thus interpretting these as vectors, these systems of linear equations are solved by:\n",
    "   \n",
    "\\begin{equation}\n",
    "R = (I-\\beta Q^{\\pi})^{-1} r\n",
    "\\end{equation}\n",
    "\n",
    "The function below does this calculation for the reward function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Construct_Rewards_For_Transitions(pi,P,beta,r):\n",
    "    Q_pi=[]\n",
    "    I=np.identity(len(pi))\n",
    "    \n",
    "    for i in range(len(pi)):\n",
    "        Q_pi.append(P[pi[i]][i])\n",
    "    Q_pi=np.matrix(Q_pi)\n",
    "    \n",
    "    R_pi = inv( I - beta * Q_pi) @ r\n",
    "    \n",
    "    return R_pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the rewards calculated above we now need to perform the policy \n",
    "improvement step. That is for each state=$x$ we solve the maximization\n",
    "\n",
    "\\begin{equation}\n",
    "\\pi_{\\text{new}}(x)\\in \\textit{argmax}_{a \\in \\mathcal A}\\;\\; r(x) + \\beta \\mathbb E_{x,a} \\Big[ R^\\pi(X' ) \\Big].\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Find_New_Policy(P,R_pi):\n",
    "    \n",
    "    new_pi=[]\n",
    "    Reward_Left = beta * ( np.matrix(P[0]) @ R_pi )\n",
    "    Reward_Right = beta * ( np.matrix(P[1]) @ R_pi )\n",
    "    Reward_Up = beta * ( np.matrix(P[2]) @ R_pi )\n",
    "    Reward_Down = beta * ( np.matrix(P[3]) @ R_pi )\n",
    "    \n",
    "    for state in range(len(pi)):\n",
    "        action=np.argmax(\n",
    "                [np.matrix.item(Reward_Left[state]),\n",
    "                 np.matrix.item(Reward_Right[state]),\n",
    "                 np.matrix.item(Reward_Up[state]),\n",
    "                 np.matrix.item(Reward_Down[state])])\n",
    "        new_pi.append(action)\n",
    "    \n",
    "    return new_pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define some arbirary policy $\\pi$ to start with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi=[0,0,0,1,0,0,0,3,0,3,0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we simply iterate on the steps above until a fixed point is reached \n",
    "\n",
    "(This takes only 4 steps or so)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 2, 0, 3, 0, 2, 2, 3, 1, 1, 0]\n",
      "[1, 1, 2, 0, 3, 0, 2, 2, 1, 1, 1, 0]\n",
      "[1, 1, 2, 0, 3, 0, 2, 2, 3, 1, 1, 0]\n",
      "[1, 1, 2, 0, 3, 0, 2, 2, 3, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "for _ in range(100):\n",
    "    R_pi = Construct_Rewards_For_Transitions(pi,P,beta,r)\n",
    "    new_pi = Find_New_Policy(P,R_pi)\n",
    "    print(new_pi)\n",
    "    if new_pi == pi :\n",
    "        break\n",
    "    else:\n",
    "        pi = new_pi\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can print the rewards assoicated with the last (optimal) policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.35613505   0.50788985   0.72484776 -10.           0.25047825\n",
      "  -20.           6.39709719   7.9896067    0.17831926   0.13306751\n",
      "   10.21393859  20.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(np.transpose(R_pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
